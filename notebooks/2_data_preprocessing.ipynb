{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2302d16",
   "metadata": {},
   "source": [
    "# Prétraitement des Données Textuelles pour la Prédiction de Sentiment\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Ce notebook couvre la phase de prétraitement des données textuelles pour notre projet d'analyse de sentiment. Suite à l'analyse exploratoire des données (EDA) réalisée dans le notebook `1_data_exploration.ipynb`, nous savons que les données sont relativement propres (pas de valeurs manquantes majeures) mais nécessitent une transformation pour être utilisables par les modèles de machine learning.\n",
    "\n",
    "## Objectifs du Prétraitement\n",
    "\n",
    "Les principaux objectifs de cette phase sont :\n",
    "1. Nettoyer le texte des commentaires (supprimer le bruit, normaliser la casse).\n",
    "2. Transformer le texte en une représentation numérique (vectorisation).\n",
    "3. Préparer le dataset pour l'entraînement du modèle en le divisant en ensembles d'entraînement et de test.\n",
    "4. Prendre en compte les observations de l'EDA, notamment la distribution des sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e586c894",
   "metadata": {},
   "source": [
    "## 2. Importation des Bibliothèques et Chargement des Données\n",
    "\n",
    "Nous importons les bibliothèques nécessaires et chargeons le dataset nettoyé si des modifications ont été faites lors de l'EDA (sinon, le dataset original)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b54406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset chargé avec succès.\n",
      "Dimensions du dataset : (499, 7)\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # Pour les expressions régulières (nettoyage de texte)\n",
    "import nltk # Pour le traitement du langage naturel (tokenisation, stopwords, etc.)\n",
    "import string # Pour la ponctuation\n",
    "from nltk.corpus import stopwords # Pour les mots vides\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer # Pour le stemming et la lemmatisation\n",
    "from sklearn.model_selection import train_test_split # Pour diviser les données\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Pour la vectorisation TF-IDF\n",
    "\n",
    "try:\n",
    "    stop_words_english = set(stopwords.words('english'))\n",
    "    # nltk.data.find('corpora/wordnet') # Vérifie si WordNet est installé (pour lemmatisation)\n",
    "except LookupError:\n",
    "    print(\"Téléchargement des ressources NLTK (stopwords, wordnet)...\")\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4') # Open Multilingual WordNet (souvent nécessaire pour WordNet)\n",
    "    stop_words_english = set(stopwords.words('english'))\n",
    "    print(\"Téléchargement terminé.\")\n",
    "\n",
    "\n",
    "# Chargement du dataset\n",
    "file_path = '../data/sentiment_analysis.csv'\n",
    "text_column = 'text'\n",
    "sentiment_column = 'sentiment'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Dataset chargé avec succès.\")\n",
    "    print(f\"Dimensions du dataset : {df.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erreur : Le fichier {file_path} n'a pas été trouvé. Placez-le dans le dossier 'data/'.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7948a8f",
   "metadata": {},
   "source": [
    "## 3. Nettoyage et Normalisation du Texte\n",
    "\n",
    "Cette section applique une série d'opérations pour nettoyer les commentaires et les préparer à la vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d813ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour nettoyer un commentaire\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str): # S'assurer que l'entrée est une chaîne\n",
    "        text = text.lower() # 1. Convertir en minuscules\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # 2. Supprimer les URLs\n",
    "        text = re.sub(r'\\@\\w+|\\#','', text) # 3. Supprimer les mentions (@) et hashtags (#)\n",
    "        text = re.sub(r'\\d+', '', text) # 4. Supprimer les chiffres\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation)) # 5. Supprimer la ponctuation\n",
    "        text = text.strip() # 6. Supprimer les espaces blancs en début et fin\n",
    "        # Suppression des espaces multiples\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "    else:\n",
    "        return \"\" # Retourner une chaîne vide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594be2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettoyage de la colonne 'text'...\n",
      "Nettoyage terminé.\n",
      "\n",
      "Exemples de commentaires après nettoyage :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What a great day!!! Looks like dream.</td>\n",
       "      <td>what a great day looks like dream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I feel sorry, I miss you here in the sea beach</td>\n",
       "      <td>i feel sorry i miss you here in the sea beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don't angry me</td>\n",
       "      <td>dont angry me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We attend in the class just for listening teac...</td>\n",
       "      <td>we attend in the class just for listening teac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Those who want to go, let them go</td>\n",
       "      <td>those who want to go let them go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   \n",
       "0              What a great day!!! Looks like dream.  \\\n",
       "1     I feel sorry, I miss you here in the sea beach   \n",
       "2                                     Don't angry me   \n",
       "3  We attend in the class just for listening teac...   \n",
       "4                  Those who want to go, let them go   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0                  what a great day looks like dream  \n",
       "1      i feel sorry i miss you here in the sea beach  \n",
       "2                                      dont angry me  \n",
       "3  we attend in the class just for listening teac...  \n",
       "4                   those who want to go let them go  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if df is not None and text_column in df.columns:\n",
    "    print(f\"Nettoyage de la colonne '{text_column}'...\")\n",
    "    df['cleaned_text'] = df[text_column].apply(clean_text)\n",
    "    print(\"Nettoyage terminé.\")\n",
    "\n",
    "    # Afficher quelques exemples après nettoyage\n",
    "    print(\"\\nExemples de commentaires après nettoyage :\")\n",
    "    display(df[[text_column, 'cleaned_text']].head())\n",
    "elif df is not None:\n",
    "    print(f\"Impossible d'appliquer le nettoyage : la colonne de texte '{text_column}' n'a pas été trouvée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8d8d1",
   "metadata": {},
   "source": [
    "### Tokenisation, Suppression des Mots Vides et Normalisation\n",
    "\n",
    "Après le nettoyage de base, nous allons :\n",
    "1. **Tokeniser** le texte (diviser en mots).\n",
    "2. Supprimer les **mots vides** (stop words) qui n'apportent pas de sens pour la classification de sentiment.\n",
    "3. Appliquer une **normalisation** pour réduire les mots à leur forme de base (stemming ou lemmatisation).\n",
    "\n",
    "Nous allons utiliser la **Lemmatisation** car elle est généralement plus précise que le stemming (elle réduit les mots à leur forme canonique en tenant compte du contexte et du vocabulaire, plutôt qu'une simple règle de troncature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24518c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du Lemmatizer (pour WordNet)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def advanced_text_processing(text):\n",
    "    if isinstance(text, str):\n",
    "        # Tokenisation\n",
    "        tokens = text.split() # Tokenisation simple par espace après nettoyage de la ponctuation\n",
    "\n",
    "        # Suppression des mots vides et lemmatisation\n",
    "        processed_tokens = [\n",
    "            lemmatizer.lemmatize(word) for word in tokens\n",
    "            if word not in stop_words_english and len(word) > 1 # Supprime stopwords et mots courts\n",
    "        ]\n",
    "\n",
    "        return \" \".join(processed_tokens)\n",
    "    return \"\"\n",
    "\n",
    "# La fonction `advanced_text_processing` prend le texte déjà nettoyé. Elle le divise en mots (`split()`), filtre les mots vides\n",
    "# (`stop_words_english`) et les mots courts, puis applique la lemmatisation (`lemmatizer.lemmatize()`) pour réduire les mots à \n",
    "# leur forme de base. Le résultat est une chaîne de texte prétraitée prête pour la vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b926c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application du traitement avancé (tokenisation, stopwords, lemmatisation) sur 'cleaned_text'...\n",
      "Traitement avancé terminé.\n",
      "\n",
      "Exemples de commentaires après traitement avancé :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what a great day looks like dream</td>\n",
       "      <td>great day look like dream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel sorry i miss you here in the sea beach</td>\n",
       "      <td>feel sorry miss sea beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dont angry me</td>\n",
       "      <td>dont angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we attend in the class just for listening teac...</td>\n",
       "      <td>attend class listening teacher reading slide n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>those who want to go let them go</td>\n",
       "      <td>want go let go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text   \n",
       "0                  what a great day looks like dream  \\\n",
       "1      i feel sorry i miss you here in the sea beach   \n",
       "2                                      dont angry me   \n",
       "3  we attend in the class just for listening teac...   \n",
       "4                   those who want to go let them go   \n",
       "\n",
       "                                      processed_text  \n",
       "0                          great day look like dream  \n",
       "1                          feel sorry miss sea beach  \n",
       "2                                         dont angry  \n",
       "3  attend class listening teacher reading slide n...  \n",
       "4                                     want go let go  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne 'cleaned_text' supprimée.\n"
     ]
    }
   ],
   "source": [
    "if df is not None and 'cleaned_text' in df.columns:\n",
    "    print(f\"Application du traitement avancé (tokenisation, stopwords, lemmatisation) sur 'cleaned_text'...\")\n",
    "    # Appliquez la fonction sur la colonne de texte nettoyée\n",
    "    df['processed_text'] = df['cleaned_text'].apply(advanced_text_processing)\n",
    "    print(\"Traitement avancé terminé.\")\n",
    "\n",
    "    # Afficher quelques exemples après traitement avancé\n",
    "    print(\"\\nExemples de commentaires après traitement avancé :\")\n",
    "    display(df[['cleaned_text', 'processed_text']].head())\n",
    "\n",
    "    df = df.drop('cleaned_text', axis=1) # Supprimez la colonne nettoyée pour économiser de l'espace mémoire\n",
    "    print(\"Colonne 'cleaned_text' supprimée.\")\n",
    "\n",
    "elif df is not None:\n",
    "     print(f\"Erreur : La colonne 'cleaned_text' n'a pas été trouvée. Assurez-vous que le nettoyage a bien été exécuté.\")\n",
    "     \n",
    "     \n",
    "#  Nous créons une nouvelle colonne `processed_text` contenant les commentaires après tokenisation,\n",
    "#  suppression des mots vides et lemmatisation. L'affichage des exemples permet de vérifier le résultat. \n",
    "#  À ce stade, la colonne `processed_text` est prête pour la vectorisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62017b7d",
   "metadata": {},
   "source": [
    "## 4. Vectorisation du Texte (TF-IDF)\n",
    "\n",
    "Nous allons convertir le texte prétraité (`processed_text`) en une représentation numérique. Nous utiliserons la technique TF-IDF (`TfidfVectorizer` de scikit-learn). TF-IDF pondère les mots en fonction de leur fréquence dans un document et de leur rareté dans l'ensemble du corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c968647d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorisation du texte en utilisant TF-IDF...\n",
      "Vectorisation terminée.\n",
      "Forme de la matrice TF-IDF (nombre de documents, taille du vocabulaire) : (499, 130)\n",
      "Forme du vecteur cible : (499,)\n",
      "\n",
      "Quelques exemples de features (mots) dans le vocabulaire TF-IDF :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['actually', 'always', 'amazing', 'baby', 'back', 'bad', 'best',\n",
       "       'better', 'big', 'bit', 'cant', 'car', 'coffee', 'come', 'coming',\n",
       "       'could', 'day', 'didnt', 'done', 'dont', 'dude', 'ever',\n",
       "       'everyone', 'family', 'feel', 'feeling', 'finally', 'find',\n",
       "       'finished', 'first'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if df is not None and 'processed_text' in df.columns and sentiment_column in df.columns:\n",
    "    print(\"Vectorisation du texte en utilisant TF-IDF...\")\n",
    "\n",
    "    # Initialisation du TfidfVectorizer\n",
    "    # min_df: ignore les termes qui apparaissent dans moins de X documents\n",
    "    # max_df: ignore les termes qui apparaissent dans plus de X% des documents\n",
    "    # ngram_range: considère des n-grammes de 1 mot (unigrammes)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, # Limite le vocabulaire aux 5000 mots/n-grammes les plus fréquents\n",
    "                                       min_df=5,         # Ignore les termes qui apparaissent dans moins de 5 documents\n",
    "                                       max_df=0.8)       # Ignore les termes qui apparaissent dans plus de 80% des documents\n",
    "\n",
    "    # Adaptez le vectoriseur sur le texte prétraité et transformez-le\n",
    "    X = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "    # La variable cible (sentiment)\n",
    "    y = df[sentiment_column]\n",
    "\n",
    "    print(\"Vectorisation terminée.\")\n",
    "    print(f\"Forme de la matrice TF-IDF (nombre de documents, taille du vocabulaire) : {X.shape}\")\n",
    "    print(f\"Forme du vecteur cible : {y.shape}\")\n",
    "\n",
    "    # Afficher les noms des features (mots) dans l'ordre\n",
    "    print(\"\\nQuelques exemples de features (mots) dans le vocabulaire TF-IDF :\")\n",
    "    display(tfidf_vectorizer.get_feature_names_out()[:30])\n",
    "\n",
    "elif df is not None:\n",
    "     print(f\"Erreur : Les colonnes 'processed_text' ou '{sentiment_column}' n'ont pas été trouvées. Vérifiez les étapes précédentes.\")\n",
    "else:\n",
    "     print(\"Erreur : Le DataFrame n'est pas chargé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d19ba",
   "metadata": {},
   "source": [
    "## 5. Division des Données (Entraînement/Test)\n",
    "\n",
    "Nous allons diviser le dataset vectorisé en un ensemble d'entraînement et un ensemble de test. L'ensemble d'entraînement sera utilisé pour entraîner le modèle, et l'ensemble de test pour évaluer ses performances sur des données invisibles.\n",
    "\n",
    "Une division typique est 80% pour l'entraînement et 20% pour le test. Nous allons également stratifier la division pour nous assurer que la proportion de chaque classe de sentiment est la même dans les ensembles d'entraînement et de test, ce qui est important étant donné le léger déséquilibre des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a0f426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division des données en ensembles d'entraînement et de test...\n",
      "Division terminée.\n",
      "Forme de l'ensemble d'entraînement (X_train) : (399, 130)\n",
      "Forme de l'ensemble de test (X_test) : (100, 130)\n",
      "Forme des labels d'entraînement (y_train) : (399,)\n",
      "Forme des labels de test (y_test) : (100,)\n",
      "\n",
      "Distribution des sentiments dans y_train :\n",
      "sentiment\n",
      "neutral     39.849624\n",
      "positive    33.333333\n",
      "negative    26.817043\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution des sentiments dans y_test :\n",
      "sentiment\n",
      "neutral     40.0\n",
      "positive    33.0\n",
      "negative    27.0\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if 'X' in locals() and 'y' in locals():\n",
    "    print(\"Division des données en ensembles d'entraînement et de test...\")\n",
    "\n",
    "    # Division en entraînement (80%) et test (20%)\n",
    "    # random_state: assure la reproductibilité de la division\n",
    "    # stratify=y: assure que les proportions des classes de sentiment sont maintenues dans les deux ensembles\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "    print(\"Division terminée.\")\n",
    "    print(f\"Forme de l'ensemble d'entraînement (X_train) : {X_train.shape}\")\n",
    "    print(f\"Forme de l'ensemble de test (X_test) : {X_test.shape}\")\n",
    "    print(f\"Forme des labels d'entraînement (y_train) : {y_train.shape}\")\n",
    "    print(f\"Forme des labels de test (y_test) : {y_test.shape}\")\n",
    "\n",
    "    # Vérifier la distribution des classes dans les ensembles\n",
    "    print(\"\\nDistribution des sentiments dans y_train :\")\n",
    "    print(y_train.value_counts(normalize=True) * 100)\n",
    "    print(\"\\nDistribution des sentiments dans y_test :\")\n",
    "    print(y_test.value_counts(normalize=True) * 100)\n",
    "\n",
    "else:\n",
    "     print(\"Erreur : Les variables X ou y n'ont pas été créées. Vérifiez l'étape de vectorisation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263211e",
   "metadata": {},
   "source": [
    "*Commentaire :* Nous utilisons `train_test_split` pour diviser les données. `test_size=0.2` alloue 20% des données au test. `random_state=42` assure que la division est la même à chaque exécution. L'argument `stratify=y` est très important ici car il maintient les proportions des classes de sentiment dans les deux ensembles, ce qui évite d'avoir un ensemble de test avec très peu d'exemples des classes minoritaires. L'affichage des formes et de la distribution des classes vérifie que la division s'est bien passée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e7f21",
   "metadata": {},
   "source": [
    "## 6. Conclusion du Prétraitement et Prochaines Étapes\n",
    "\n",
    "Dans ce notebook, nous avons appliqué une série d'étapes pour prétraiter les commentaires textuels :\n",
    "- Nettoyage de base (minuscules, suppression de ponctuation, URLs, chiffres, etc.).\n",
    "- Tokenisation, suppression des mots vides et lemmatisation.\n",
    "- Transformation du texte prétraité en vecteurs numériques à l'aide de TF-IDF.\n",
    "- Division du dataset vectorisé en ensembles d'entraînement et de test stratifiés.\n",
    "\n",
    "Le résultat de cette phase est :\n",
    "- `X_train` et `y_train` : Les données d'entraînement (features vectorisées et labels).\n",
    "- `X_test` et `y_test` : Les données de test (features vectorisées et labels).\n",
    "- Le `tfidf_vectorizer` entraîné, qui sera nécessaire pour prétraiter de nouveaux commentaires pour la prédiction.\n",
    "\n",
    "Ces ensembles sont maintenant prêts à être utilisés pour entraîner différents modèles de machine learning pour la classification de sentiment.\n",
    "\n",
    "### Prochaines Étapes :\n",
    "\n",
    "La phase suivante consistera à :\n",
    "1. Sélectionner et entraîner plusieurs modèles de classification (par exemple, Naïve Bayes, Régression Logistique, SVM, potentiellement des modèles de deep learning).\n",
    "2. Évaluer les performances de chaque modèle sur l'ensemble de test en utilisant des métriques appropriées (Accuracy, Precision, Recall, F1-score, Matrice de confusion), en portant une attention particulière aux classes minoritaires en raison du léger déséquilibre.\n",
    "3. Comparer les modèles et sélectionner le \"meilleur\" modèle en justifiant ce choix par les performances et les contraintes d'intégration web (vitesse d'inférence, taille du modèle)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
